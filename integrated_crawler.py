#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ–éŸ³ç”¨æˆ·ä¿¡æ¯æ•´åˆæŠ“å–å·¥å…·
åŠŸèƒ½ï¼šé¡ºåºå¤„ç†urls_config.txtä¸­çš„é“¾æ¥ï¼Œå…ˆé…ç½®Cookieï¼Œå†æŠ“å–ç”¨æˆ·ä¿¡æ¯
å®ç°æµç¨‹ï¼š
1. æå–ç¬¬ä¸€æ¡é“¾æ¥ -> é…ç½®Cookie -> æŠ“å–ç”¨æˆ·ä¿¡æ¯
2. æå–ç¬¬äºŒæ¡é“¾æ¥ -> é…ç½®Cookie -> æŠ“å–ç”¨æˆ·ä¿¡æ¯
3. ä»¥æ­¤ç±»æ¨...
"""

import json
import os
import re
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Union

# å¯¼å…¥Cookieé…ç½®ç®¡ç†å™¨
try:
    from batch_config_cookie import BatchDouyinCookieManager
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥ batch_config_cookie æ¨¡å—ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨")
    sys.exit(1)

# å¯¼å…¥douyin-4çš„ç”¨æˆ·ä¿¡æ¯è·å–æ¨¡å—
try:
    # æ·»åŠ douyin-4çš„libç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
    douyin4_lib_path = os.path.join(os.path.dirname(__file__), '..', 'douyin-4', 'lib')
    douyin4_path = os.path.join(os.path.dirname(__file__), '..', 'douyin-4')
    sys.path.insert(0, douyin4_lib_path)
    sys.path.insert(0, douyin4_path)
    
    import request
    import cookies  
    import util
    from get_user_info import DouyinUserInfo
    
    # å¯¼å…¥æ‰€éœ€å‡½æ•°
    Request = request.Request
    get_cookie_dict = cookies.get_cookie_dict
    str_to_path = util.str_to_path
    url_redirect = util.url_redirect
    
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥douyin-4æ¨¡å—: {e}")
    print("è¯·ç¡®ä¿douyin-4ç›®å½•å­˜åœ¨ä¸”åŒ…å«æ‰€éœ€æ–‡ä»¶")
    sys.exit(1)


class IntegratedDouyinCrawler:
    """æ•´åˆçš„æŠ–éŸ³çˆ¬è™« - å…ˆé…ç½®Cookieï¼Œå†æŠ“å–ç”¨æˆ·ä¿¡æ¯"""
    
    def __init__(self):
        # åˆå§‹åŒ–Cookieé…ç½®ç®¡ç†å™¨
        self.cookie_manager = BatchDouyinCookieManager()
        
        # é…ç½®æ–‡ä»¶è·¯å¾„
        self.urls_config_file = os.path.join(os.path.dirname(__file__), "urls_config.txt")
        self.output_dir = "integrated_output"
        self.log_file = "integrated_crawler_log.txt"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        
        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            "total_urls": 0,
            "cookie_success": 0,
            "cookie_failed": 0,
            "crawl_success": 0,
            "crawl_failed": 0,
            "start_time": None,
            "end_time": None,
            "results": []
        }
        
        # å½“å‰ä½¿ç”¨çš„ç”¨æˆ·ä¿¡æ¯è·å–å™¨
        self.user_getter = None
    
    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] [INTEGRATED] {message}"
        print(log_entry)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
    
    def read_urls_config(self) -> List[str]:
        """è¯»å–URLsé…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.urls_config_file):
            self.log(f"URLsé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.urls_config_file}", "ERROR")
            return []
        
        urls = []
        try:
            with open(self.urls_config_file, "r", encoding="utf-8") as f:
                content = f.read()
            
            lines = content.split('\n')
            for line_num, line in enumerate(lines, 1):
                line = line.strip()
                
                # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Šè¡Œ
                if not line or line.startswith('#'):
                    continue
                
                # å¤„ç†å•è¡Œå¤šä¸ªé“¾æ¥ï¼ˆé€—å·æˆ–åˆ†å·åˆ†éš”ï¼‰
                if ',' in line or ';' in line:
                    split_urls = re.split(r'[,;]', line)
                    for url in split_urls:
                        url = url.strip()
                        if url and self.cookie_manager.validate_douyin_url(url):
                            urls.append(url)
                        elif url:
                            self.log(f"ç¬¬{line_num}è¡Œå‘ç°æ— æ•ˆé“¾æ¥: {url}", "WARNING")
                else:
                    # å•ä¸ªé“¾æ¥
                    if self.cookie_manager.validate_douyin_url(line):
                        urls.append(line)
                    else:
                        self.log(f"ç¬¬{line_num}è¡Œå‘ç°æ— æ•ˆé“¾æ¥: {line}", "WARNING")
            
            self.log(f"ä»é…ç½®æ–‡ä»¶è¯»å–åˆ° {len(urls)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            return urls
            
        except Exception as e:
            self.log(f"è¯»å–URLsé…ç½®æ–‡ä»¶å¤±è´¥: {e}", "ERROR")
            return []
    
    def configure_cookie_for_url(self, url: str, index: int, total: int) -> bool:
        """ä¸ºæŒ‡å®šURLé…ç½®Cookie"""
        self.log(f"[{index}/{total}] å¼€å§‹ä¸ºé“¾æ¥é…ç½®Cookie: {url}")
        
        try:
            # ä½¿ç”¨Cookieç®¡ç†å™¨çš„å•ä¸ªURLå¤„ç†æ–¹æ³•
            result = self.cookie_manager.process_single_url(url, index, total)
            
            if result["success"]:
                self.log(f"[{index}/{total}] âœ… Cookieé…ç½®æˆåŠŸ")
                self.stats["cookie_success"] += 1
                return True
            else:
                self.log(f"[{index}/{total}] âŒ Cookieé…ç½®å¤±è´¥: {result['message']}", "ERROR")
                self.stats["cookie_failed"] += 1
                return False
                
        except Exception as e:
            self.log(f"[{index}/{total}] âŒ Cookieé…ç½®å¼‚å¸¸: {e}", "ERROR")
            self.stats["cookie_failed"] += 1
            return False
    
    def load_cookie_from_config(self):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½Cookieå­—ç¬¦ä¸²"""
        cookie_config_file = self.cookie_manager.cookie_config_file
        
        if not os.path.exists(cookie_config_file):
            self.log(f"Cookieé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {cookie_config_file}", "ERROR")
            return None
        
        try:
            with open(cookie_config_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            for line in lines:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # è§£æcookieé…ç½®
                if line.startswith('cookie='):
                    cookie = line[7:].strip()  # ç§»é™¤'cookie='å‰ç¼€
                    if cookie and cookie != "your_cookie_string_here":
                        self.log("âœ… ä»é…ç½®æ–‡ä»¶è¯»å–åˆ°æœ‰æ•ˆCookie")
                        return cookie
                
                elif line.startswith('browser='):
                    browser = line[8:].strip()  # ç§»é™¤'browser='å‰ç¼€
                    if browser in ['chrome', 'edge']:
                        self.log(f"âš ï¸ å‘ç°æµè§ˆå™¨é…ç½®ï¼Œä½†éœ€è¦å…·ä½“çš„Cookieå­—ç¬¦ä¸²: {browser}")
            
            self.log("âŒ é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„Cookieå­—ç¬¦ä¸²", "ERROR")
            return None
            
        except Exception as e:
            self.log(f"è¯»å–Cookieé…ç½®æ–‡ä»¶å¤±è´¥: {e}", "ERROR")
            return None
    
    def crawl_user_info(self, url: str, index: int, total: int) -> Optional[Dict]:
        """æŠ“å–ç”¨æˆ·ä¿¡æ¯"""
        self.log(f"[{index}/{total}] å¼€å§‹æŠ“å–ç”¨æˆ·ä¿¡æ¯: {url}")
        
        try:
            # è·å–å½“å‰Cookie
            cookie = self.load_cookie_from_config()
            if not cookie:
                self.log(f"[{index}/{total}] âŒ æ— æ³•è·å–æœ‰æ•ˆCookieï¼Œè·³è¿‡æŠ“å–", "ERROR")
                return None
            
            # åˆå§‹åŒ–æˆ–é‡æ–°åˆå§‹åŒ–ç”¨æˆ·ä¿¡æ¯è·å–å™¨
            self.user_getter = DouyinUserInfo(cookie)
            
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_info = self.user_getter.get_user_info_from_url(url)
            
            if user_info:
                self.log(f"[{index}/{total}] âœ… ç”¨æˆ·ä¿¡æ¯è·å–æˆåŠŸ")
                self.log(f"[{index}/{total}]    ğŸ‘¤ æ˜µç§°: {user_info.get('nickname', 'æœªçŸ¥')}")
                self.log(f"[{index}/{total}]    ğŸ“ ç­¾å: {user_info.get('signature', 'æ— ')}")
                self.log(f"[{index}/{total}]    ğŸŒ åœ°åŒº: {user_info.get('ip_location', 'æœªçŸ¥')}")
                self.log(f"[{index}/{total}]    ğŸ‘¥ ç²‰ä¸: {user_info.get('follower_count', 0)}")
                
                self.stats["crawl_success"] += 1
                return user_info
            else:
                self.log(f"[{index}/{total}] âŒ ç”¨æˆ·ä¿¡æ¯è·å–å¤±è´¥", "ERROR")
                self.stats["crawl_failed"] += 1
                return None
                
        except Exception as e:
            self.log(f"[{index}/{total}] âŒ æŠ“å–ç”¨æˆ·ä¿¡æ¯å¼‚å¸¸: {e}", "ERROR")
            self.stats["crawl_failed"] += 1
            return None
    
    def save_user_info(self, user_info: Dict, url: str, index: int) -> Optional[str]:
        """ä¿å­˜ç”¨æˆ·ä¿¡æ¯åˆ°æ–‡ä»¶"""
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            nickname = user_info.get('nickname', 'æœªçŸ¥ç”¨æˆ·')
            safe_nickname = str_to_path(nickname)  # è½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å
            
            # é™åˆ¶æ˜µç§°é•¿åº¦
            if len(safe_nickname) > 20:
                safe_nickname = safe_nickname[:20]
            
            filename = f"{timestamp}_{index:03d}_{safe_nickname}.json"
            filepath = os.path.join(self.output_dir, filename)
            
            # å‡†å¤‡è¾“å‡ºæ•°æ®
            output_data = {
                'extraction_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'source_url': url,
                'processing_index': index,
                'user_info': user_info
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            self.log(f"âœ… ç”¨æˆ·ä¿¡æ¯å·²ä¿å­˜: {filename}")
            return filepath
            
        except Exception as e:
            self.log(f"âŒ ä¿å­˜ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {e}", "ERROR")
            return None
    
    def process_single_url(self, url: str, index: int, total: int) -> Dict:
        """å¤„ç†å•ä¸ªURLçš„å®Œæ•´æµç¨‹ï¼šé…ç½®Cookie -> æŠ“å–ç”¨æˆ·ä¿¡æ¯"""
        result = {
            "url": url,
            "index": index,
            "timestamp": datetime.now().isoformat(),
            "cookie_success": False,
            "crawl_success": False,
            "user_info": None,
            "filepath": None,
            "error_message": None
        }
        
        self.log(f"\n{'='*60}")
        self.log(f"[{index}/{total}] å¼€å§‹å¤„ç†é“¾æ¥: {url}")
        self.log(f"{'='*60}")
        
        # ç¬¬ä¸€æ­¥ï¼šé…ç½®Cookie
        self.log(f"[{index}/{total}] æ­¥éª¤1: é…ç½®Cookie")
        cookie_success = self.configure_cookie_for_url(url, index, total)
        result["cookie_success"] = cookie_success
        
        if not cookie_success:
            result["error_message"] = "Cookieé…ç½®å¤±è´¥"
            self.log(f"[{index}/{total}] âŒ ç”±äºCookieé…ç½®å¤±è´¥ï¼Œè·³è¿‡ç”¨æˆ·ä¿¡æ¯æŠ“å–")
            return result
        
        # æ·»åŠ å»¶è¿Ÿï¼Œç¡®ä¿Cookieé…ç½®ç”Ÿæ•ˆ
        self.log(f"[{index}/{total}] ç­‰å¾…2ç§’ï¼Œç¡®ä¿Cookieé…ç½®ç”Ÿæ•ˆ...")
        time.sleep(2)
        
        # ç¬¬äºŒæ­¥ï¼šæŠ“å–ç”¨æˆ·ä¿¡æ¯
        self.log(f"[{index}/{total}] æ­¥éª¤2: æŠ“å–ç”¨æˆ·ä¿¡æ¯")
        user_info = self.crawl_user_info(url, index, total)
        
        if user_info:
            result["crawl_success"] = True
            result["user_info"] = user_info
            
            # ç¬¬ä¸‰æ­¥ï¼šä¿å­˜ç”¨æˆ·ä¿¡æ¯
            self.log(f"[{index}/{total}] æ­¥éª¤3: ä¿å­˜ç”¨æˆ·ä¿¡æ¯")
            filepath = self.save_user_info(user_info, url, index)
            result["filepath"] = filepath
            
            if filepath:
                self.log(f"[{index}/{total}] âœ… é“¾æ¥å¤„ç†å®Œæˆ")
            else:
                result["error_message"] = "ä¿å­˜æ–‡ä»¶å¤±è´¥"
                self.log(f"[{index}/{total}] âš ï¸ æŠ“å–æˆåŠŸä½†ä¿å­˜å¤±è´¥")
        else:
            result["error_message"] = "ç”¨æˆ·ä¿¡æ¯æŠ“å–å¤±è´¥"
            self.log(f"[{index}/{total}] âŒ ç”¨æˆ·ä¿¡æ¯æŠ“å–å¤±è´¥")
        
        return result
    
    def run_integrated_crawl(self, delay_seconds: int = 5) -> bool:
        """è¿è¡Œæ•´åˆçˆ¬è™«çš„ä¸»æµç¨‹"""
        self.log("=== æŠ–éŸ³ç”¨æˆ·ä¿¡æ¯æ•´åˆæŠ“å–å·¥å…·å¯åŠ¨ ===")
        
        # è¯»å–URLs
        urls = self.read_urls_config()
        if not urls:
            self.log("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æŠ–éŸ³é“¾æ¥ï¼Œè¯·æ£€æŸ¥ urls_config.txt æ–‡ä»¶", "ERROR")
            return False
        
        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats["total_urls"] = len(urls)
        self.stats["start_time"] = datetime.now().isoformat()
        
        self.log(f"å‡†å¤‡æŒ‰é¡ºåºå¤„ç† {len(urls)} ä¸ªé“¾æ¥")
        
        # é¡ºåºå¤„ç†æ¯ä¸ªURL
        for index, url in enumerate(urls, 1):
            result = self.process_single_url(url, index, len(urls))
            self.stats["results"].append(result)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªURLï¼Œæ·»åŠ å»¶è¿Ÿ
            if index < len(urls):
                self.log(f"ç­‰å¾… {delay_seconds} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªé“¾æ¥...")
                time.sleep(delay_seconds)
        
        # å®Œæˆå¤„ç†
        self.stats["end_time"] = datetime.now().isoformat()
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_final_report()
        
        return self.stats["crawl_success"] > 0
    
    def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š"""
        stats = self.stats
        duration = 0
        
        if stats["start_time"] and stats["end_time"]:
            start = datetime.fromisoformat(stats["start_time"])
            end = datetime.fromisoformat(stats["end_time"])
            duration = (end - start).total_seconds()
        
        self.log("\n" + "="*70)
        self.log("æ•´åˆæŠ“å–å®Œæˆç»Ÿè®¡æŠ¥å‘Š")
        self.log("="*70)
        self.log(f"æ€»é“¾æ¥æ•°: {stats['total_urls']}")
        self.log(f"Cookieé…ç½®æˆåŠŸ: {stats['cookie_success']} âœ…")
        self.log(f"Cookieé…ç½®å¤±è´¥: {stats['cookie_failed']} âŒ")
        self.log(f"ç”¨æˆ·ä¿¡æ¯æŠ“å–æˆåŠŸ: {stats['crawl_success']} âœ…")
        self.log(f"ç”¨æˆ·ä¿¡æ¯æŠ“å–å¤±è´¥: {stats['crawl_failed']} âŒ")
        self.log(f"æ€»å¤„ç†æ—¶é•¿: {duration:.2f} ç§’")
        
        if stats["total_urls"] > 0:
            cookie_rate = (stats["cookie_success"] / stats["total_urls"] * 100)
            crawl_rate = (stats["crawl_success"] / stats["total_urls"] * 100)
            self.log(f"Cookieé…ç½®æˆåŠŸç‡: {cookie_rate:.1f}%")
            self.log(f"ä¿¡æ¯æŠ“å–æˆåŠŸç‡: {crawl_rate:.1f}%")
        
        # æ˜¾ç¤ºæˆåŠŸæŠ“å–çš„ç”¨æˆ·
        successful_results = [r for r in stats["results"] if r["crawl_success"]]
        if successful_results:
            self.log("\næˆåŠŸæŠ“å–çš„ç”¨æˆ·:")
            for result in successful_results:
                user_info = result.get("user_info", {})
                nickname = user_info.get("nickname", "æœªçŸ¥")
                follower_count = user_info.get("follower_count", 0)
                filepath = result.get("filepath", "æœªä¿å­˜")
                self.log(f"  - {nickname} (ç²‰ä¸: {follower_count}, æ–‡ä»¶: {os.path.basename(filepath)})")
        
        # æ˜¾ç¤ºå¤±è´¥çš„é“¾æ¥
        failed_results = [r for r in stats["results"] if not r["crawl_success"]]
        if failed_results:
            self.log("\nå¤„ç†å¤±è´¥çš„é“¾æ¥:")
            for result in failed_results:
                error_msg = result.get("error_message", "æœªçŸ¥é”™è¯¯")
                self.log(f"  - {result['url']}: {error_msg}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_data = {
            'summary': {
                'total_urls': stats['total_urls'],
                'cookie_success': stats['cookie_success'],
                'cookie_failed': stats['cookie_failed'],
                'crawl_success': stats['crawl_success'],
                'crawl_failed': stats['crawl_failed'],
                'processing_time_seconds': round(duration, 2),
                'start_time': stats['start_time'],
                'end_time': stats['end_time']
            },
            'results': stats['results']
        }
        
        report_filename = f"integrated_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_filepath = os.path.join(self.output_dir, report_filename)
        
        try:
            with open(report_filepath, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            self.log(f"ğŸ“‹ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
        except Exception as e:
            self.log(f"âš ï¸ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}", "WARNING")
        
        self.log("="*70)
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
        help_text = f"""
=== æŠ–éŸ³ç”¨æˆ·ä¿¡æ¯æ•´åˆæŠ“å–å·¥å…·ä½¿ç”¨è¯´æ˜ ===

åŠŸèƒ½ï¼š
- ä» {self.urls_config_file} é¡ºåºè¯»å–æŠ–éŸ³ç”¨æˆ·é“¾æ¥
- å¯¹æ¯ä¸ªé“¾æ¥ä¾æ¬¡æ‰§è¡Œï¼š
  1. é…ç½®Cookie
  2. æŠ“å–ç”¨æˆ·ä¿¡æ¯
  3. ä¿å­˜åˆ°æ–‡ä»¶
- ç¡®ä¿æ¯ä¸ªé“¾æ¥éƒ½æœ‰æœ€æ–°çš„Cookie

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç¼–è¾‘ {self.urls_config_file} æ–‡ä»¶ï¼Œæ·»åŠ æŠ–éŸ³ç”¨æˆ·é“¾æ¥
2. åœ¨æµè§ˆå™¨ä¸­ç™»å½•æŠ–éŸ³è´¦å·
3. ä»¥ç®¡ç†å‘˜æƒé™è¿è¡Œï¼špython integrated_crawler.py
4. æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ç»“æœæ–‡ä»¶

è¾“å‡ºæ–‡ä»¶ï¼š
- {self.output_dir}/: ç”¨æˆ·ä¿¡æ¯JSONæ–‡ä»¶
- integrated_report_*.json: å¤„ç†ç»Ÿè®¡æŠ¥å‘Š
- {self.log_file}: è¿è¡Œæ—¥å¿—

æ³¨æ„äº‹é¡¹ï¼š
- Windowsç³»ç»Ÿéœ€è¦ç®¡ç†å‘˜æƒé™è¿è¡Œ
- ç¡®ä¿å·²å®‰è£… rookiepy åº“
- éœ€è¦å…ˆåœ¨æµè§ˆå™¨ä¸­ç™»å½•æŠ–éŸ³è´¦å·
- ç¨‹åºä¼šåœ¨æ¯ä¸ªé“¾æ¥é—´æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
"""
        print(help_text)


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        arg = sys.argv[1].lower()
        
        if arg in ["-h", "--help", "help"]:
            crawler = IntegratedDouyinCrawler()
            crawler.show_help()
            return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = IntegratedDouyinCrawler()
    
    try:
        # è¿è¡Œæ•´åˆçˆ¬è™«
        success = crawler.run_integrated_crawl(delay_seconds=5)  # è®¾ç½®5ç§’å»¶è¿Ÿ
        
        if success:
            print("\nâœ… æ•´åˆæŠ“å–å®Œæˆï¼")
            print(f"è¾“å‡ºç›®å½•: {crawler.output_dir}")
            print(f"æ—¥å¿—æ–‡ä»¶: {crawler.log_file}")
        else:
            print("\nâŒ æ•´åˆæŠ“å–å¤±è´¥ï¼")
            print("è¯·æ£€æŸ¥:")
            print("1. urls_config.txt æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆé“¾æ¥")
            print("2. æ˜¯å¦åœ¨æµè§ˆå™¨ä¸­ç™»å½•äº†æŠ–éŸ³è´¦å·")
            print("3. æ˜¯å¦ä»¥ç®¡ç†å‘˜æƒé™è¿è¡Œç¨‹åº")
            print("4. æ˜¯å¦å®‰è£…äº† rookiepy åº“")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹")
        crawler.log("ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹", "WARNING")
    except Exception as e:
        print(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        crawler.log(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", "ERROR")


if __name__ == "__main__":
    main()
